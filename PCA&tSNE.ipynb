{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20793f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import numpy as np\n",
    "np.random.seed(2022)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import scipy.stats as stats\n",
    "\n",
    "# PCA imports\n",
    "from sklearn.decomposition import PCA\n",
    "#t-SNE import\n",
    "from sklearn import manifold, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71834cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "n_samples = 1500\n",
    "S_points, S_color = datasets.make_s_curve(n_samples, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5552e026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x205b374cbe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "#ax.scatter(x_r[0],x_r[1],x_r[2])\n",
    "\n",
    "ax.scatter(S_points.T[0],S_points.T[1],S_points.T[2],c=S_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b8093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Explained:  [0.69133634 0.18724734 0.12141632]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'PC2  18.7%')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components = 3) # this defines the model\n",
    "\n",
    "# PCA vizualization\n",
    "X_pca = pca.fit_transform(S_points) # transform the data into PCA space\n",
    "print('Variance Explained: ', pca.explained_variance_ratio_)\n",
    "\n",
    "fig =plt.figure()\n",
    "ax=fig.add_subplot()\n",
    "ax.scatter(X_pca[:,0],X_pca[:,1],c=S_color)\n",
    "ax.set_aspect('equal')\n",
    "plt.xlabel('PC1  '+str(100*round(pca.explained_variance_ratio_[0],3))+'%')\n",
    "plt.ylabel('PC2  '+str(100*round(pca.explained_variance_ratio_[1],3))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95af028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'PC2  12.1%')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig =plt.figure()\n",
    "ax=fig.add_subplot()\n",
    "ax.scatter(X_pca[:,0],X_pca[:,2],c=S_color)\n",
    "ax.set_aspect('equal')\n",
    "plt.xlabel('PC1  '+str(100*round(pca.explained_variance_ratio_[0],3))+'%')\n",
    "plt.ylabel('PC2  '+str(100*round(pca.explained_variance_ratio_[2],3))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c9bacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x205b5273a60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_sne = manifold.TSNE(\n",
    "    n_components=2,\n",
    "    learning_rate=\"auto\",\n",
    "    perplexity=10,\n",
    "    n_iter=250,\n",
    "    init=\"random\",\n",
    "    random_state=rng,)\n",
    "\n",
    "S_t_sne = t_sne.fit_transform(S_points)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.scatter(S_t_sne.T[0],S_t_sne.T[1],c=S_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ad5847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.0, 25.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's create two clusters set\n",
    "\n",
    "x_1=np.random.normal(loc=1,scale=2.0,size=100)\n",
    "x_2=np.random.normal(loc=1,scale=2.0,size=100)\n",
    "x_3=np.random.normal(loc=1,scale=0.7,size=100)\n",
    "\n",
    "y_1=np.random.normal(loc=1,scale=2.0,size=100)\n",
    "y_2=np.random.normal(loc=1,scale=2.0,size=100)\n",
    "y_3=np.random.normal(loc=-5,scale=0.7,size=100)\n",
    "\n",
    "x=np.array([x_1,x_2,x_1+2*x_2+x_3])\n",
    "y=np.array([y_1,y_2,y_3])\n",
    "\n",
    "z=np.concatenate((x,y),axis=1)\n",
    "\n",
    "clr=100*['b']+100*['r']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(z[0],z[1],z[2],c=clr)\n",
    "\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$x_3$')\n",
    "\n",
    "ax.set_xlim((-10,25))\n",
    "ax.set_ylim((-10,25))\n",
    "ax.set_zlim((-10,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b47a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sne = manifold.TSNE(\n",
    "    n_components=2,\n",
    "    learning_rate=\"auto\",\n",
    "    perplexity=100,\n",
    "    n_iter=250,\n",
    "    init=\"random\",\n",
    "    random_state=rng,\n",
    ")\n",
    "\n",
    "z_t_sne = t_sne.fit_transform(z.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4415e553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.0, 25.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.scatter(z_t_sne.T[0],z_t_sne.T[1],c=clr)\n",
    "\n",
    "\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "\n",
    "ax.set_xlim((-10,25))\n",
    "ax.set_ylim((-10,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41941bdf",
   "metadata": {},
   "source": [
    "#### t-SNE \n",
    "\n",
    "##### t-distributed stochastic neighbor embedding\n",
    "\n",
    "The intuition about t-SNE is that you want to reduce the dimensionality in a way that preserves the distances between the points.\n",
    "\n",
    "Example: Let's move a village on a 2D grid, to a village with only 1 street.\n",
    "\n",
    "How do you keep old neighbors in 2D close to each other in 1D street?\n",
    "\n",
    "Example on board.\n",
    "\n",
    "We can do it in the statistical sense. \n",
    "\n",
    "#### Side note: The curse of dimensionality\n",
    "\n",
    "In 1D each points on a grid has 2 closest neighbors.\n",
    "\n",
    "In 2D there are 4 closest ppoints. In 3D 6, etc. \n",
    "\n",
    "In Many dimensional space, say 1000 dimensions, each point has 2000 neighbors. \n",
    "If we have more dimensions than points, each point is a neighbor to any othe point. \n",
    "The mappings will be very difficult. \n",
    "\n",
    "Back to t-SNE. \n",
    "\n",
    "Let's have $N$ points $x_i$. \n",
    "\n",
    "Let's create a probability distribution to be a neighbor, based on the distances between the points. \n",
    "\n",
    "Let's say a gaussian:\n",
    "\n",
    "$P_{i|j}=\\frac{e^{-\\frac{|x_i-x_j|^2}{2\\sigma_i^2}}}{\\sum_{k}e^{-\\frac{|x_i-x_k|^2}{2\\sigma_i^2}}}$\n",
    "\n",
    "here $\\sigma_i$ is a parameter, such that the entropy is preset to a chosen value. This is a parameter of the model. \n",
    "It is related to the $\\bf{perplexity}$. This is a measure of how many neighbors do we anticipate for each point in the data.\n",
    "\n",
    "We will symmetrize this probability:\n",
    "\n",
    "$P_{ij}=\\frac{P_{i|j}+P_{j|i}}{2N}$\n",
    "\n",
    "#### Perplexity in t-SNE\n",
    "\n",
    "This is a measure of how many neighbors do we anticipate for each point in the data. \n",
    "\n",
    "The result depend on this parameter, which is not a parameter of the data itself. \n",
    "\n",
    "The perplexity is the effective number of neighbors calculated from the entropy:\n",
    "\n",
    "$N_{eff}=2^{-\\sum P_{ij}logP_{ij}}$\n",
    "\n",
    "#### Hyperparameters in ML\n",
    "\n",
    "Parameters of the method, set the rate of learning. Can have influence on the results. \n",
    "\n",
    "Continue with the procedure:\n",
    "\n",
    "We will try to reduce the dimensionality of the data, i.e. create new points $y_i$ in lower dimensional space, say 2D. \n",
    "We will do that in  a way that will preserve the distribution of distances, but with broader tails. \n",
    "\n",
    "$\\bf{This\\ means\\ there\\ will\\ be\\ more\\ larger\\ distances\\ in\\ the\\ reduced\\ dimensionality\\ map.\\ As\\ if\\ we\\ push\\ the\\ points\\ that\\ are\\ appart,\\ further\\ appart.}$ \n",
    "\n",
    "We will use the t-distribution for $y_i$. \n",
    "\n",
    "$Q_{ij}=\\frac{(1+|y_i-y_j|^2)^{-1}}{\\sum_{k\\neq l}(1+|y_k-y_l|^2)^{-1}}$\n",
    "\n",
    "We will try to make these distributions by minimizing the \"distance\" bewteen the distributions.\n",
    "\n",
    "$\\bf{Kullback-Leibler\\ divergence}$\n",
    "\n",
    "$KL(P||Q)=\\sum_{i\\neq j}P_{ij}ln\\frac{P_{ij}}{Q_{ij}}$\n",
    "\n",
    "\n",
    "If $P=Q$ (as a distribution) then $KL=0$. \n",
    "\n",
    "How do we minimize this? As always, we take the derivatives over the new points to be zero: $\\frac{dKL}{dy_i}=0$.\n",
    "\n",
    "We can use Steepest descent, for example. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
